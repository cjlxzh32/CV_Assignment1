{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO/VsAojRIgVs5ig9D6uF5H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"RVF_YO_o4FZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9OP9ihE4C5H"},"outputs":[],"source":["!pip install torch torchvision matplotlib scipy h5py tqdm"]},{"cell_type":"code","source":["!git clone https://github.com/magicleap/SuperGluePretrainedNetwork.git\n","%cd SuperGluePretrainedNetwork"],"metadata":{"id":"k4Ny0Pwi4hnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import cv2 as cv\n","import torch\n","from pathlib import Path\n","import json\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import shutil\n","from datetime import datetime\n","import glob\n","import zipfile\n","import time\n","import gc\n","\n","sys.path.append('/content/SuperGluePretrainedNetwork')\n","from models.matching import Matching\n","from models.utils import (compute_pose_error, compute_epipolar_error,\n","                          estimate_pose, make_matching_plot,\n","                          error_colormap, AverageTimer, pose_auc)"],"metadata":{"id":"gbe6trU0EQVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adjust_K_for_orientation(img_shape, K):\n","    \"\"\"\n","    Adjust camera matrix based on image orientation\n","    \"\"\"\n","    h, w = img_shape[:2]\n","    if w > h:\n","        K_adjusted = np.array([\n","            [K[1, 1], 0, K[1, 2]],  # fy->fx, cy->cx\n","            [0, K[0, 0], K[0, 2]],\n","            [0, 0, 1]\n","        ])\n","    else:\n","        K_adjusted = K.copy()\n","\n","    return K_adjusted"],"metadata":{"id":"k0_lVTiiZJPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adjust_K_for_scaling(K, scale_factor):\n","    \"\"\"\n","    Adjust camera matrix for image scaling\n","    \"\"\"\n","    K_scaled = K.copy()\n","    K_scaled[0, 0] *= scale_factor  # fx\n","    K_scaled[1, 1] *= scale_factor  # fy\n","    K_scaled[0, 2] *= scale_factor  # cx\n","    K_scaled[1, 2] *= scale_factor  # cy\n","\n","    return K_scaled"],"metadata":{"id":"-GXGv172ZKBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adjust_camera_matrix(img_shape, K, scale_factor=1.0):\n","    K_oriented = adjust_K_for_orientation(img_shape, K)\n","    K_adjusted = adjust_K_for_scaling(K_oriented, scale_factor)\n","\n","    return K_adjusted"],"metadata":{"id":"simyu6Q_ZNr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = {\n","    'superpoint': {\n","        'nms_radius': 3,\n","        'keypoint_threshold': 0.005,\n","        'max_keypoints': 100000\n","    },\n","    'superglue': {\n","        'weights': 'outdoor',\n","        'sinkhorn_iterations': 30,\n","        'match_threshold': 0.3,\n","    }\n","}\n","\n","def enhance_sculpture_details(img):\n","    # Convert to LAB color space\n","    if len(img.shape) == 3:\n","        lab = cv.cvtColor(img, cv.COLOR_BGR2LAB)\n","        l, a, b = cv.split(lab)\n","    else:\n","        l = img.copy()\n","        lab = None\n","\n","    # Apply CLAHE to enhance contrast\n","    clahe = cv.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n","    cl = clahe.apply(l)\n","\n","    # Apply sharpening\n","    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n","    cl = cv.filter2D(cl, -1, kernel)\n","\n","    # Edge enhancement\n","    edges = cv.Canny(cl, 50, 150)\n","    edges_dilated = cv.dilate(edges, np.ones((3,3), np.uint8))\n","\n","    cl_float = cl.astype(np.float32)\n","    edges_float = edges_dilated.astype(np.float32)\n","    cl = cv.addWeighted(cl_float, 1.0, edges_float, 0.3, 0)\n","    cl = np.clip(cl, 0, 255).astype(np.uint8)\n","\n","    if lab is not None:\n","        enhanced_lab = cv.merge((cl, a, b))\n","        enhanced = cv.cvtColor(enhanced_lab, cv.COLOR_LAB2BGR)\n","    else:\n","        enhanced = cl\n","\n","    return enhanced\n","\n","def resize_image(img, max_size=1024):\n","    h, w = img.shape[:2]\n","    scale = min(max_size/w, max_size/h)\n","    if scale < 1.0:\n","        new_size = (int(w*scale), int(h*scale))\n","        resized = cv.resize(img, new_size, interpolation=cv.INTER_AREA)\n","        return resized, scale\n","    else:\n","        return img, 1.0\n","\n","def compute_reprojection_error(P1, P2, pts1, pts2, pts3d):\n","    pts3d_h = np.hstack([pts3d, np.ones((pts3d.shape[0], 1))])\n","    proj1 = (P1 @ pts3d_h.T).T\n","    proj2 = (P2 @ pts3d_h.T).T\n","    proj1 = proj1[:, :2] / proj1[:, 2:3]\n","    proj2 = proj2[:, :2] / proj2[:, 2:3]\n","    err1 = np.linalg.norm(proj1 - pts1, axis=1)\n","    err2 = np.linalg.norm(proj2 - pts2, axis=1)\n","    rmse = np.sqrt(np.mean(np.hstack([err1, err2]) ** 2))\n","    return float(rmse)\n","\n","def visualize_camera_trajectory(cam_centers, save_path=None):\n","    fig = plt.figure(figsize=(10, 8))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    ax.scatter(cam_centers[:, 0], cam_centers[:, 1], cam_centers[:, 2],\n","              c='blue', marker='o', s=50, label='Camera Centers')\n","\n","    ax.plot(cam_centers[:, 0], cam_centers[:, 1], cam_centers[:, 2],\n","           c='red', linewidth=2, label='Camera Path')\n","\n","    ax.set_xlabel('X')\n","    ax.set_ylabel('Y')\n","    ax.set_zlabel('Z')\n","    ax.set_title('Camera Trajectory')\n","\n","    if save_path:\n","        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n","    return fig, ax\n","\n","def load_images_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png'), max_size=1024):\n","    images = {}\n","    image_paths = []\n","    scales = {}\n","\n","    for ext in extensions:\n","        image_paths.extend(sorted(glob.glob(os.path.join(folder_path, f'*{ext}'))))\n","\n","    if not image_paths:\n","        raise ValueError(f\"No images found in {folder_path}\")\n","\n","    print(f\"Found {len(image_paths)} images in {folder_path}\")\n","\n","    for path in tqdm(image_paths, desc=\"Loading images\"):\n","        img = cv.imread(path)\n","        if img is not None:\n","            name = os.path.splitext(os.path.basename(path))[0]\n","            resized_img, scale = resize_image(img, max_size=max_size)\n","            images[name] = resized_img\n","            scales[name] = scale\n","\n","    return images, image_paths, scales\n","\n","def extract_features_with_superpoint(images, device='cuda', batch_size=4):\n","    \"\"\"Use SuperPoint to extract features\"\"\"\n","    matching = Matching(config).eval().to(device)\n","    keypoints = {}\n","    descriptors = {}\n","    scores = {}  # Add keypoint scores\n","\n","    image_names = list(images.keys())\n","    for i in range(0, len(image_names), batch_size):\n","        batch_names = image_names[i:i+batch_size]\n","\n","        for name in tqdm(batch_names, desc=f\"Extracting features\"):\n","            img = images[name]\n","            enhanced = enhance_sculpture_details(img)\n","            if len(enhanced.shape) == 3:\n","                gray = cv.cvtColor(enhanced, cv.COLOR_BGR2GRAY)\n","            else:\n","                gray = enhanced.copy()\n","\n","            gray_tensor = torch.from_numpy(gray).float().to(device)[None, None] / 255.0\n","            with torch.no_grad():\n","                pred = matching.superpoint({'image': gray_tensor})\n","                kpts = pred['keypoints'][0].cpu().numpy()\n","                desc = pred['descriptors'][0].cpu().numpy()\n","                score = pred['scores'][0].cpu().numpy()  # Save keypoint scores\n","\n","                # Combine points and sizes (x, y, size)\n","                kp_with_size = np.hstack([kpts, 8 * np.ones((kpts.shape[0], 1))])\n","\n","                keypoints[name] = kp_with_size\n","                descriptors[name] = desc.T  # Transpose to match OpenCV format\n","                scores[name] = score\n","\n","            torch.cuda.empty_cache()\n","            del gray_tensor, pred\n","            gc.collect()\n","\n","    return keypoints, descriptors, scores\n","\n","def match_features_with_superglue(images, keypoints, descriptors, scores, device='cuda'):\n","    \"\"\"Use SuperGlue for feature matching\"\"\"\n","    matching = Matching(config).eval().to(device)\n","    image_names = sorted(list(images.keys()))\n","    matches_dict = {}\n","\n","    for i in tqdm(range(len(image_names) - 1), desc=\"Matching image pairs\"):\n","        name0, name1 = image_names[i], image_names[i+1]\n","        img0, img1 = images[name0], images[name1]\n","\n","        # Convert to grayscale\n","        if len(img0.shape) == 3:\n","            gray0 = cv.cvtColor(img0, cv.COLOR_BGR2GRAY)\n","        else:\n","            gray0 = img0.copy()\n","\n","        if len(img1.shape) == 3:\n","            gray1 = cv.cvtColor(img1, cv.COLOR_BGR2GRAY)\n","        else:\n","            gray1 = img1.copy()\n","\n","        data = {\n","            'image0': torch.from_numpy(gray0).float().to(device)[None, None] / 255.0,\n","            'image1': torch.from_numpy(gray1).float().to(device)[None, None] / 255.0,\n","            'keypoints0': torch.from_numpy(keypoints[name0][:, :2]).float().to(device)[None],\n","            'keypoints1': torch.from_numpy(keypoints[name1][:, :2]).float().to(device)[None],\n","            'descriptors0': torch.from_numpy(descriptors[name0].T).float().to(device)[None],\n","            'descriptors1': torch.from_numpy(descriptors[name1].T).float().to(device)[None],\n","            'scores0': torch.from_numpy(scores[name0]).float().to(device)[None],\n","            'scores1': torch.from_numpy(scores[name1]).float().to(device)[None]\n","        }\n","\n","        with torch.no_grad():\n","            pred = matching(data)\n","\n","        matches = pred['matches0'][0].cpu().numpy()\n","        confidence = pred['matching_scores0'][0].cpu().numpy()\n","\n","        good_matches = []\n","        for i, m in enumerate(matches):\n","            if m != -1 and confidence[i] > 0.5:  # 仅保留高置信度匹配\n","                good_matches.append(cv.DMatch(i, int(m), float(1.0 - confidence[i])))\n","\n","        matches_dict[(name0, name1)] = {\n","            'matches': good_matches,\n","            'confidence': confidence\n","        }\n","\n","        print(f\"Matched {name0}-{name1}: {len(good_matches)} matches\")\n","\n","        torch.cuda.empty_cache()\n","        del data, pred\n","        gc.collect()\n","\n","    return matches_dict\n","\n","def estimate_pose_from_matches(kps1, kps2, matches, K, K1=None, K2=None, scale1=1.0, scale2=1.0):\n","    \"\"\"\n","    Compute F, E, R, t using matched keypoints\n","    \"\"\"\n","    if K1 is None:\n","        K1 = K.copy()\n","    if K2 is None:\n","        K2 = K.copy()\n","\n","    if isinstance(matches[0], cv.DMatch):\n","        # DMatch objects\n","        pts1 = np.float32([kps1[m.queryIdx][:2] for m in matches])\n","        pts2 = np.float32([kps2[m.trainIdx][:2] for m in matches])\n","    else:\n","        # Index pairs [idx1, idx2]\n","        pts1 = np.float32([kps1[m[0]][:2] for m in matches])\n","        pts2 = np.float32([kps2[m[1]][:2] for m in matches])\n","\n","    # Apply scale if needed\n","    if scale1 != 1.0:\n","        pts1 *= scale1\n","    if scale2 != 1.0:\n","        pts2 *= scale2\n","\n","    F, mask = cv.findFundamentalMat(\n","        pts1, pts2, cv.FM_RANSAC,\n","        ransacReprojThreshold=1.0,\n","        confidence=0.999\n","    )\n","\n","    if F is None or mask is None or np.sum(mask) < 8:\n","        print(f\"[ERR] findFundamentalMat失败: 内点太少 ({np.sum(mask) if mask is not None else 0})\")\n","        return None\n","\n","    inlier_mask = mask.ravel() == 1\n","    inlier_pts1 = pts1[inlier_mask]\n","    inlier_pts2 = pts2[inlier_mask]\n","    inlier_matches = [m for i, m in enumerate(matches) if inlier_mask[i]]\n","    inlier_count = len(inlier_pts1)\n","    inlier_ratio = inlier_count / len(pts1)\n","\n","    if inlier_count < 8:\n","        print(f\"[ERR] RANSAC后内点太少 ({inlier_count})\")\n","        return None\n","\n","    E, mask = cv.findEssentialMat(pts1, pts2, K1, method=cv.RANSAC, prob=0.999, threshold=1.0)\n","\n","    try:\n","        success, R, t, mask_pose = cv.recoverPose(E, inlier_pts1, inlier_pts2, K1)\n","    except cv.error as e:\n","        print(f\"[ERR] recoverPose失败: {e}\")\n","        return None\n","\n","    if not success or np.sum(mask_pose) < 5:\n","        print(f\"[ERR] recoverPose失败: 有效点太少 ({np.sum(mask_pose)})\")\n","        return None\n","\n","    P1 = K1 @ np.hstack((np.eye(3), np.zeros((3, 1))))\n","    P2 = K2 @ np.hstack((R, t))\n","    pts4d = cv.triangulatePoints(P1, P2, inlier_pts1.T, inlier_pts2.T)\n","    pts3d = (pts4d[:3] / pts4d[3]).T\n","    reproj_error = compute_reprojection_error(P1, P2, inlier_pts1, inlier_pts2, pts3d)\n","\n","    result = {\n","        \"F\": F,\n","        \"E\": E,\n","        \"R\": R,\n","        \"t\": t,\n","        \"inlier_matches\": inlier_matches,\n","        \"inliers1\": inlier_pts1,\n","        \"inliers2\": inlier_pts2,\n","        \"inlier_count\": inlier_count,\n","        \"inlier_ratio\": inlier_ratio,\n","        \"reproj_error\": reproj_error,\n","        \"pts3d\": pts3d\n","    }\n","\n","    return result\n","\n","def visualize_matches(img1, img2, kps1, kps2, matches, out_path=None):\n","    kp_cv1 = [cv.KeyPoint(x=f[0], y=f[1], size=8.0) for f in kps1]\n","    kp_cv2 = [cv.KeyPoint(x=f[0], y=f[1], size=8.0) for f in kps2]\n","\n","    img_matches = cv.drawMatches(\n","        img1, kp_cv1, img2, kp_cv2, matches, None,\n","        flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n","    )\n","\n","    font = cv.FONT_HERSHEY_SIMPLEX\n","    cv.putText(img_matches, f\"Matches: {len(matches)}\", (10, 30), font, 0.6, (0, 255, 0), 2)\n","    cv.putText(img_matches, f\"Inliers: {len([m for m in matches if m.distance < 0.5])}\",\n","               (10, 60), font, 0.6, (0, 255, 0), 2)\n","\n","    if out_path:\n","        cv.imwrite(out_path, img_matches)\n","\n","    return img_matches\n","\n","def process_image_sequence(images, keypoints, descriptors, scores, K, output_dir, original_scales=None):\n","    matches_dir = os.path.join(output_dir, \"matches\")\n","    viz_dir = os.path.join(output_dir, \"viz\")\n","    poses_dir = os.path.join(output_dir, \"poses\")\n","\n","    os.makedirs(matches_dir, exist_ok=True)\n","    os.makedirs(viz_dir, exist_ok=True)\n","    os.makedirs(poses_dir, exist_ok=True)\n","\n","    image_names = sorted(list(images.keys()))\n","    summary_records = []\n","    pose_list = []\n","\n","    match_results = match_features_with_superglue(images, keypoints, descriptors, scores)\n","\n","    for i in tqdm(range(len(image_names) - 1), desc=\"Processing matches\"):\n","        name1, name2 = image_names[i], image_names[i+1]\n","        pair_key = (name1, name2)\n","\n","        if pair_key not in match_results:\n","            print(f\"[WARN] No matches found for {name1}-{name2}, skipping\")\n","            continue\n","\n","        matches = match_results[pair_key]['matches']\n","\n","        if len(matches) < 30:\n","            print(f\"[WARN] Too few matches ({len(matches)}) for {name1}-{name2}, skipping\")\n","            continue\n","\n","        scale1 = original_scales[name1] if original_scales else 1.0\n","        scale2 = original_scales[name2] if original_scales else 1.0\n","\n","        img1_shape, img2_shape = images[name1].shape, images[name2].shape\n","        K1 = adjust_camera_matrix(img1_shape, K.copy(), scale1)\n","        K2 = adjust_camera_matrix(img2_shape, K.copy(), scale2)\n","\n","        print(f\"[INFO] Original K:\\n{K}\")\n","        print(f\"[INFO] Adjusted K1 (shape={img1_shape[:2]}, scale={scale1}):\\n{K1}\")\n","        print(f\"[INFO] Adjusted K2 (shape={img2_shape[:2]}, scale={scale2}):\\n{K2}\")\n","\n","        pose_result = estimate_pose_from_matches(\n","          keypoints[name1], keypoints[name2], matches, K,\n","          K1=K1, K2=K2\n","        )\n","\n","        if pose_result is None:\n","            print(f\"[WARN] Failed to estimate pose for {name1}-{name2}, skipping\")\n","            continue\n","\n","        F, E, R, t = pose_result[\"F\"], pose_result[\"E\"], pose_result[\"R\"], pose_result[\"t\"]\n","        inliers1, inliers2 = pose_result[\"inliers1\"], pose_result[\"inliers2\"]\n","        inlier_matches = pose_result[\"inlier_matches\"]\n","        inlier_count = pose_result[\"inlier_count\"]\n","        inlier_ratio = pose_result[\"inlier_ratio\"]\n","        reproj_error = pose_result[\"reproj_error\"]\n","        pts3d = pose_result[\"pts3d\"]\n","\n","\n","        try:\n","            img1, img2 = images[name1], images[name2]\n","            viz_img = visualize_matches(\n","                img1, img2, keypoints[name1][:, :2], keypoints[name2][:, :2],\n","                inlier_matches,\n","                out_path=os.path.join(viz_dir, f\"{name1}_{name2}_matches.jpg\")\n","            )\n","        except Exception as e:\n","            print(f\"[WARN] Failed to create visualization for {name1}-{name2}: {e}\")\n","            viz_path = os.path.join(viz_dir, f\"{name1}_{name2}_matches.jpg\")\n","            cv.imwrite(viz_path, viz_img)\n","        except Exception as e:\n","            print(f\"[WARN] Failed to create visualization for {name1}-{name2}: {e}\")\n","\n","        match_data = {\n","            \"image_1\": name1,\n","            \"image_2\": name2,\n","            \"num_matches\": len(matches),\n","            \"inlier_count\": inlier_count,\n","            \"inlier_ratio\": float(inlier_ratio),\n","            \"reproj_rmse\": float(reproj_error),\n","            \"F\": F.tolist(),\n","            \"E\": E.tolist(),\n","            \"R\": R.tolist(),\n","            \"t\": t.flatten().tolist()\n","        }\n","\n","        match_path = os.path.join(matches_dir, f\"{name1}_{name2}_pose.json\")\n","        with open(match_path, \"w\") as f:\n","            json.dump(match_data, f, indent=2)\n","\n","        inlier_path = os.path.join(matches_dir, f\"{name1}_{name2}_inliers.npz\")\n","        np.savez_compressed(inlier_path, pts1=inliers1, pts2=inliers2)\n","        pose_path = os.path.join(poses_dir, f\"{name1}_{name2}.txt\")\n","        np.savetxt(pose_path, np.hstack([R.flatten(), t.flatten()])[None], fmt=\"%.6f\")\n","        summary_records.append([\n","            \"sculpture\",\n","            f\"{name1}-{name2}\",\n","            len(matches),\n","            inlier_count,\n","            round(float(inlier_ratio), 3),\n","            round(float(reproj_error), 3),\n","            \"OK\"\n","        ])\n","\n","        pose_list.append((R, t))\n","\n","        print(f\"[DONE] {name1}-{name2}: inliers={inlier_count}, ratio={inlier_ratio:.2f}, RMSE={reproj_error:.2f}\")\n","        gc.collect()\n","\n","    non_sequential_dir = os.path.join(output_dir, \"non_sequential\")\n","    os.makedirs(os.path.join(non_sequential_dir, \"matches\"), exist_ok=True)\n","    os.makedirs(os.path.join(non_sequential_dir, \"poses\"), exist_ok=True)\n","\n","    summary_path = os.path.join(output_dir, \"superglue_summary.csv\")\n","    with open(summary_path, \"w\", newline=\"\") as f:\n","        import csv\n","        writer = csv.writer(f)\n","        writer.writerow([\"scene\", \"pair\", \"num_matches\", \"inlier_count\", \"inlier_ratio\", \"reproj_error\", \"status\"])\n","        writer.writerows(summary_records)\n","\n","    return summary_records, pose_list\n","\n","def accumulate_camera_poses(pose_list):\n","    cam_centers = []\n","    T_world = np.eye(4)\n","    cam_centers.append(T_world[:3, 3].copy())\n","\n","    for R, t in pose_list:\n","        T_rel = np.eye(4)\n","        T_rel[:3, :3] = R\n","        T_rel[:3, 3] = t.flatten()\n","\n","        T_rel_inv = np.eye(4)\n","        T_rel_inv[:3, :3] = R.T\n","        T_rel_inv[:3, 3] = -R.T @ t.flatten()\n","\n","        T_world = T_world @ T_rel_inv\n","        cam_centers.append(T_world[:3, 3].copy())\n","\n","    return np.array(cam_centers)\n","\n","def save_features_to_drive(features_dir, keypoints, descriptors, scores=None):\n","    os.makedirs(features_dir, exist_ok=True)\n","\n","    for name, kps in keypoints.items():\n","        np.save(os.path.join(features_dir, f\"{name}_keypoints.npy\"), kps)\n","        np.save(os.path.join(features_dir, f\"{name}_descriptors.npy\"), descriptors[name])\n","        if scores is not None:\n","            np.save(os.path.join(features_dir, f\"{name}_scores.npy\"), scores[name])\n","\n","    print(f\"Saved features to {features_dir}\")\n","    return features_dir\n","\n","def create_downloadable_zip(output_dir, zip_name=\"superglue_results.zip\"):\n","    zip_path = os.path.join(output_dir, zip_name)\n","    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for root, dirs, files in os.walk(output_dir):\n","            for file in files:\n","                if file != zip_name:\n","                    file_path = os.path.join(root, file)\n","                    arcname = os.path.relpath(file_path, output_dir)\n","                    zipf.write(file_path, arcname)\n","\n","    print(f\"Created downloadable ZIP at {zip_path}\")\n","    return zip_path"],"metadata":{"id":"64kpxIXr_lGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SCENE_NAME = 'sculpture'\n","MAX_IMAGE_SIZE = 1024\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","BASE_DIR = '/content/drive/MyDrive/sfm'\n","IMAGE_DIR = '/content/drive/MyDrive/sfm/reaching_high_sculpture_fps2'\n","#IMAGE_DIR = '/content/drive/MyDrive/sfm/arch'\n","OUTPUT_DIR = '/content/drive/MyDrive/sfm/results/sculpture'\n","\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","if DEVICE == 'cuda':\n","    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","else:\n","    print(\"CUDA not available, using CPU. This will be much slower.\")\n","\n","print(f\"Looking for images in {IMAGE_DIR}...\")\n","images, image_paths, original_scales = load_images_from_folder(IMAGE_DIR, max_size=MAX_IMAGE_SIZE)\n","\n","if not images:\n","    raise ValueError(\"No images found! Please upload images first.\")\n","\n","print(f\"Successfully loaded {len(images)} images\")\n","sample_image = list(images.values())[0]\n","print(f\"Sample image shape: {sample_image.shape}\")\n","\n","print(\"Extracting SuperPoint features...\")\n","start_time = time.time()\n","keypoints, descriptors, scores = extract_features_with_superpoint(images, device=DEVICE)\n","feature_time = time.time() - start_time\n","print(f\"Feature extraction took {feature_time:.2f} seconds\")\n","\n","features_dir = os.path.join(OUTPUT_DIR, 'features')\n","save_features_to_drive(features_dir, keypoints, descriptors, scores)\n","\n","calib_path = os.path.join(BASE_DIR, 'iphone_calibration.npz')\n","print(f\"Loading camera calibration from {calib_path}\")\n","data = np.load(calib_path, allow_pickle=True)\n","K = data[\"K\"]\n","print(K)\n","\n","print(\"Matching features and estimating poses...\")\n","start_time = time.time()\n","summary_records, pose_list = process_image_sequence(images, keypoints, descriptors, scores, K, OUTPUT_DIR, original_scales)\n","matching_time = time.time() - start_time\n","print(f\"Feature matching and pose estimation took {matching_time:.2f} seconds\")\n","\n","if pose_list:\n","    print(\"Computing camera trajectory...\")\n","    cam_centers = accumulate_camera_poses(pose_list)\n","    np.save(os.path.join(OUTPUT_DIR, 'camera_trajectory.npy'), cam_centers)\n","    os.makedirs(os.path.join(OUTPUT_DIR, 'viz'), exist_ok=True)\n","    fig, ax = visualize_camera_trajectory(cam_centers,\n","                        save_path=os.path.join(OUTPUT_DIR, 'viz', 'camera_trajectory.png'))\n","    plt.show()\n","else:\n","    print(\"No poses estimated, cannot compute trajectory.\")\n","\n","zip_path = create_downloadable_zip(OUTPUT_DIR)\n","\n","try:\n","    from google.colab import files\n","    files.download(zip_path)\n","    print(f\"You can download the results from: {zip_path}\")\n","except:\n","    print(f\"Results saved to: {zip_path}\")\n","\n","successful_pairs = len(summary_records)\n","if successful_pairs > 0:\n","    avg_inliers = np.mean([r[3] for r in summary_records])\n","    avg_ratio = np.mean([r[4] for r in summary_records])\n","    avg_error = np.mean([r[5] for r in summary_records])\n","\n","    print(\"\\n=== SuperGlue Matching Results ===\")\n","    print(f\"Total image pairs processed: {successful_pairs}\")\n","    print(f\"Average inliers per pair: {avg_inliers:.1f}\")\n","    print(f\"Average inlier ratio: {avg_ratio:.3f}\")\n","    print(f\"Average reprojection error: {avg_error:.3f} pixels\")\n","    print(f\"Results saved to {OUTPUT_DIR}\")\n","    print(f\"ZIP file created at {zip_path}\")\n","else:\n","    print(\"\\n=== No successful matches found ===\")\n","    print(\"Please check your images and try again with different parameters.\")\n","\n","print(\"\\nProcessing complete!\")"],"metadata":{"id":"okZgtnBdQSeQ"},"execution_count":null,"outputs":[]}]}